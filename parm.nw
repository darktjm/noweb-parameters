\documentclass{article}
\usepackage{noweb}
\usepackage{hyperref}

\begin{document}

\title{Parameterized Chunks for noweb}
\author{Thomas J. Moore}
\date{Version 1.01\\23 July, 2013}
\maketitle

This document is placed in the public domain by its author.

\tableofcontents

\section{Introduction}

I like to consolidate code as much as possible.  I do this in C using
either subroutines or preprocessor macros.  The macros are more useful
when the number of inputs and/or outputs are large, or using a macro
is significantly faster.  The subroutines cannot even be used when
doing something like defining a large number of similar symbols, or
providing a more convenient way to define data tables.  The
preprocessor can usually accomplish what I need, but the price is that
the debugger cannot debug code defined by the preprocessor.  In
addition, there are some languages which lack a preprocessor.  As
such, the original Web system had macros with arguments, and I would
like something similar.

I propose the following syntax changes:

\begin{itemize}

\item A chunk name used in a chunk definition line may contain
parameter definitions in parentheses.  All parameter names begin with
an at-sign (@).  The only characters forbidden in parameter names are
close-parentheses and invalid NoWeb syntax.  Thus the parameter name
is terminated by its closing parenthesis.  Parameter definition syntax
within in-line code is ignored.

For example:
\begin{quote}
\begin{verbatim}
@<<macro (@arg1) with -(@arg2)- (but not [[(@arg3)]])>>=
text
@@
\end{verbatim}
\end{quote}

The parameters are [[@arg1]] and [[@arg2]].

\item No chunk name may begin with an at-sign (@).  That is, the names
starting with an at-sign are reserved for parameter names.

\item Chunk references beginning with an at-sign (@) expand to the
parameter with that name, as defined by the closest parent taking a
parameter of that name.  When weaved, such references are not linked
anywhere, and they are not added to the index.  When tangled, the
[[-L]] option is ignored.

For example:

\begin{quote}
\begin{verbatim}
@<<macro (@arg)>>=
text @<<@arg>>
@<<other>
@@
@<<macro2 (@arg)>>=
text2 @<<@arg>>
@<<other>>
@@
@<<other>>=
@<<@arg>>
@@
\end{verbatim}
\end{quote}

When expanded within [[macro]], both [[@arg]] and [[other]] will
expand to [[macro]]'s argument.  When expanded within [[macro2]], both
[[@arg]] and [[other]] will expand to [[macro2]]'s argument.

\item A chunk reference consisting of the name of a chunk taking
parameters, with all parameter references replaced by NoWeb in-line
code segments, represents an expansion of the reference with the
parameters taking the value of the code within the in-line code
segments in their respective positions.  No expansion is done within
the code segments except for parameter references; these may be used
to pass parameters on to another macro.

For example:

\begin{quote}
\begin{verbatim}
@<<macro (@arg)>>=
text @<<@arg>>
@<<other>
@@
@<<macro2 (@arg)>>=
@<<macro @[[and @<<@arg>>]]>>
@@
@<<macro3 (@arg)>>=
@<<macro4>>
@
<<macro4>>=
can reference @<<@arg>> of macro3.
@
@<<caller>>=
text @<<macro @[[text2]]>> @<<macro2 @[[text3]]>> @<<macro3 @[[text3]]>>
@@
\end{verbatim}
\end{quote}

The [[caller]] chunk expands to [[text text text2 text text3]].

\item When more than one parameterized chunk could be used to replace
a reference, the one with the least parameter replacements is chosen.
For equal numbers, the one which does not contain the earliest
parameter replacement is chosen.

For example:

\begin{quote}
\begin{verbatim}
@<<macro (@arg1) and (@arg2)>>=
text @<<@arg1>> blah @<<@arg2>>
@@
@<<macro @[[1]] and (@arg)>>=
text blah blah @<<@arg>>
@@
@<<macro (@arg) and @[[2]]>>=
text blah blah blah @<<@arg>>
@@
@<<*>>=
@<<macro @[[a]] and @[[b]]>> -- one blah
@<<macro @[[1]] and @[[b]]>> -- two blahs
@<<macro @[[a]] and @[[2]]>> -- three blahs
@<<macro @[[1]] and @[[2]]>> -- two blahs and a warning
@@
\end{verbatim}
\end{quote}

\end{itemize}

\section{Tangling}

Tangling these enhancements is the (somewhat) easy part.  Rather than
create a new tangler, the NoWeb pipeline is used.  Also, rather than
modifying the NoWeb tangler, a filter is created.  A filter is less
efficient than simply replacing the tangler, but it is compatible with
both NoWeb 2 and 3.

While I would prefer to write this in C, there are valid arguments for
writing this in perl instead%
\footnote{On the other hand, C++ provides most of the needed
functionality in the standard template library, so perhaps one day I
will rewrite this in C++.}%
.  If I use C, I will either have to develop some string processing
and hash table routines, or I will have to use an existing library,
such as GLib.  While I am not too concerned about non-POSIX systems,
regular expression support is not necessarily present on non-POSIX
systems, either, unless something like GLib is used.  The problem with
GLib is that it is not as commonly available as perl, and is also not
consistently easy to find (e.g. [[pkg-config]] is not universally
available, and the token for [[GLib]] is inconsistent), and requires
work to ensure that the correct versions are available.  In contrast,
as long as I do not use any esoteric libraries or advanced features,
perl should work with most installations without too much trouble.

%\lstset{language=perl}
<<Common Perl Prefix>>=
#!/bin/sh
#!perl
# GENERATED FILE:  DO NOT EDIT OR READ THIS FILE
# Instead, read or edit the NoWeb file(s) from which this was generated.
eval 'exec perl -x -f "$0" "$@"'
  if 0;

use strict;
@

<<nt-parm>>=
<<Common Perl Prefix>>

<<[[nt-parm]] local definitions>>

<<Gather code chunks from NoWeb pipeline>>

<<Mangle and dump pipeline>>
@

Tangling a parameterized chunk is done by duplicating the chunk
definition for every unique reference, replacing parameters as
necessary.  Since the above definition allows chunks expanded within
the parameterized chunk (recursively) to also expand the same
parameters, they may need to be duplicated as well.  Chunk references
in the unduplicated code do not need to be modified, since the newly
created chunk names match their usage.  In fact, the original input
can be safely dumped unmodified to the output.

All definition text is slurped into a single array.  Since we only
care about chunk definition and reference lines, array indices are
only incremented when such lines are found.  A separate hash stores,
for each unique name, a list of all definitions' text array indicies.

When duplicating a definition, the original file and line location are
needed.  When parsing a reference, the original file and line location
may be needed for error messages.  Rather than store these in separate
locations for each, a couple of arrays are added, and the file and
line for any particular definition or usage text array element is
stored in the side arrays with the same index.

<<[[nt-parm]] local definitions>>=
my (@notext, @textfile, @textline, %chunks, %parmchunks);
@

<<Gather code chunks from NoWeb pipeline>>=
my $lineno = 1; my $file = '<stdin>';
my ($codeline, $codefile, $curchunk);
push @notext, '';
while(<STDIN>) {
  print $_;
   if(/^\@begin code( |$)/) {
    $codeline = $lineno;
    $codefile = $file;
    push @notext, $_;
  } elsif(/^\@defn (.*)$/) {
    unless($codeline) {
      print STDERR "$file:$lineno: invalid chunk syntax\n";
      exit 1;
    }
    $curchunk = $1;
    $notext[$#notext] .= $_;
    $textfile[$#notext] = $file;
    $textline[$#notext] = $lineno;
    <<Store [[$curchunk]] info>>
    push @notext, '';
  } elsif(/^\@end code( |$)/) {
    unless($curchunk) {
      print STDERR "$file:$lineno: invalid chunk syntax\n";
      exit 1;
    }
    $notext[$#notext] .= $_;
    <<Finish off [[$curchunk]]>>
    push @notext, '';
    $codeline = $codefile = $curchunk = undef;
  } elsif(/^\@use .*$/) {
    if($curchunk) {
      push @notext, $_;
      $textfile[$#notext] = $file;
      $textline[$#notext] = $lineno;
      push @notext, '';
    }
  } elsif(/^\@fatal( |$)/) {
    exit 1;
  } elsif(/^\@(index |)nl( |$)/) {
    $lineno++;
    $notext[$#notext] .= $_ if $codeline;
  } elsif(/^\@file (.*)$/) {
    $file = $1;
    $lineno = 1;
    $notext[$#notext] .= $_ if $codeline;
  } elsif(/^\@line (.*)$/) {
    $lineno = $1 - 1;
    $notext[$#notext] .= $_ if $codeline;
  } else {
    $notext[$#notext] .= $_ if $codeline;
  }
}
@

The information stored for each chunk definition is the array index of
the starting and ending line.  This is stored as two array entries for
each chunk.

<<Store [[$curchunk]] info>>=
$chunks{$curchunk} = [] if not $chunks{$curchunk};
push @{$chunks{$curchunk}}, $#notext;
@

<<Finish off [[$curchunk]]>>=
push @{$chunks{$curchunk}}, $#notext;
@

Since normal code syntax is used for parameter values, special care
must be taken to distinguish them.  This is done by first scanning for
a chunk which exactly matches the reference.  If this is not found,
parameterized functions are searched, with the most explicit one
matching.  For the second step, a filter is applied to remove in-line
code and parameter definitions, creating the the key for a hash of
stripped names.  This reduces the number of chunk names that need to
be checked every time a potential expansion is found.

<<[[nt-parm]] local definitions>>=
<<[[strip_chunkname]]>>
@

<<[[strip_chunkname]]>>=
# note: $parm_re's first capture group is the parameter
#       $parm_re's second cpature group is for recursion (ignore!)
my $parm_re = '(\(\@[^)]*\)|(?<!@)\[\[((?:[^][]|\[(?-1)\])*)\]\])';
sub strip_chunkname($)
{
  my $n = shift;
  $n =~ s/$parm_re/(@)/g;
  return $n;
}
@

<<Store [[$curchunk]] info>>=
my $strip = strip_chunkname $curchunk;
$parmchunks{$strip} = {} if not $parmchunks{$strip};
${$parmchunks{$strip}}{$curchunk} = 1;
@

Once all has been collected, the duplicated chunks can be dumped.
This is done by iterating through the array, looking for [[@use]]es.
Any [[@use]] which is not already present in [[%chunks]] is stripped of
parameter values, and checked against [[%parmchunks]].  If it matches,
each possible expansion is checked, counting the number of parameter
substitutions.  The name with the fewest substitutions is retained.
If two names have an equal number of substitutions, the one which has
in-line code at the first position where the two differ is chosen, and
a warning is issued.

<<Mangle and dump pipeline>>=
DUMP: for(my $i = 0; $i <= $#notext; $i++) {
  my $u = $notext[$i];
  next if $u !~ /^\@use (.*)$/;
  my $curchunk = $1;
  next if $chunks{$curchunk};
  my ($best, %best_parms) = find_best_def(idx_to_loc($i), $curchunk);
  next unless $best;
  <<Dump best definition match for [[$curchunk]]>>
}
@

<<[[nt-parm]] local definitions>>=
<<[[find_best_def]]>>

sub idx_to_loc($) {
  my $idx = $_[0];
  return "$textfile[$idx]:$textline[$idx]: ";
}
@

<<[[find_best_def]]>>=
sub find_best_def($$) {
  my ($loc, $curchunk) = @_;
  my $pchunks = $parmchunks{strip_chunkname $curchunk};
  return if not $pchunks;
  my ($best, $best_len, %best_parms, @best_parms_loc);
  $best_len = length $curchunk;
  my @ambig;
  PCHUNK: foreach my $may (keys %$pchunks) {
    my $len = 0;
    my (%may_parms, @may_parms_loc);
    my $mayrest = $may;
    my $currest = $curchunk;
    my $parm_loc = 0;
    while($mayrest =~ /$parm_re(.*)/) {
      $parm_loc++;
      my $mayp = $1;
      $mayrest = $3;
      $currest =~ /$parm_re(.*)/;
      my $curp = $1;
      $currest = $3;
      next if $curp eq $mayp;
      next PCHUNK if substr($mayp, 0, 1) eq "[" or substr($curp, 0, 1) eq "(";
      $len++;
      $may_parms{substr($mayp, 1, -1)} = substr($curp, 2, -2);
      push @may_parms_loc, $parm_loc;
    }
    next if $len > $best_len;
    if($len == $best_len) {
      $ambig[0] = $best if $#ambig == 0;
      push @ambig, $may;
      for(my $i = 0; $i <= $#may_parms_loc; $i++) {
        next PCHUNK if $may_parms_loc[$i] < $best_parms_loc[$i];
        last if $may_parms_loc[$i] > $best_parms_loc[$i];
      }
    } else {
      @ambig = undef;
    }
    $best = $may;
    $best_len = $len;
    %best_parms = (%may_parms);
    @best_parms_loc = (@may_parms_loc);
  }
  if($#ambig > 0) {
    print STDERR $loc . "ambiguous expansion of @<<$curchunk@>>:\n";
    foreach my $may (@ambig) {
      print STDERR "  @<<$may@>>";
      print STDERR " (chosen)" if $may eq $best;
      print STDERR "\n";
    }
  }
  return ($best, %best_parms);
}
@

To dump a chunk, any [[@use]] references within the chunk must be
checked.  Those which refer to a parameter are replaced by the
parameter's value.  Those which refer to a chunk with parameter
expansions cause that chunk to be duplicated, as well, with the root
chunk name as a prefix, separated from the real name by something that
is unlikely to occur in real names ([[@<<@>>]]).  The current set of
parameters is tracked througout this process, so that parameter
references (even within parameter values) are properly expanded.
Naturally, the top-level reference can't use a parameter reference,
since no parameters are defined yet.

<<Dump best definition match for [[$curchunk]]>>=
foreach my $v (values %best_parms) {
  foreach my $bad_val ($v =~ /(?<!@)(@<<@.*?(?<!@)>>)/) {
    # since we're not notangle, we have to expand every single chunk
    # so raising an error on unknown parm is not possible
    # for now, just go ahead and leave unexpanded
    #print "$textfile[$i]:$textline[$i]: " .
    #      "undefined parameter $bad_val\n";
    last; # next DUMP;
  }
}
print dump_def($curchunk, $curchunk . '@<<@>>', $best, %best_parms);
@

<<[[nt-parm]] local definitions>>=
sub dump_def($$$%) {
  my ($new, $prefix, $old, %parms) = @_;
  my ($ret, $subdefs);
  my $chunk = $chunks{$old};
  $chunks{$new} = [];
  for(my $c = 0; $c <= $#$chunk; $c += 2) {
    my $i = $$chunk[$c];
    my $last = $$chunk[$c + 1];
    $ret .= "\@file $textfile[$i]\n\@line " . ($textline[$i] + 1) . "\n";
    $ret .= substr($notext[$i], 0, -(length($old) + 1)) . $new . "\n";
    for($i++; $i <= $last; $i++) {
      my $t = $notext[$i];
      if($t !~ /^\@use (.*)$/) {
        $ret .= $t;
      } else {
        if(substr($1, 0, 1) eq '@') {
	  # note: no newlines supported in parm vals due to syntax
	  if($parms{$1}) {
	    $ret .= "\@text $parms{$1}\n";
	  } else {
	    # again, we're not notangle, so can't raise errors here
	    #print STDERR "$textfile[$i]:$textline[$i]: " .
	    #             "undefined parameter @<<$1@>>\n" unless $parms{$1};
	    # but we can force notangle to raise an error:
	    $ret .= $t;
	  }
	} else {
	  my $new_chunk = $1;
	  my ($best, %best_parms) = find_best_def(idx_to_loc($i), $new_chunk);
	  if($best) {
	    my %new_parms = (%parms);
	    my $parms_changed;
	    foreach my $p (keys %best_parms) {
	      my $v = $best_parms{$p};
	      my $newv = '';
	      my $vrest = $v;
	      while($vrest =~ /(.*?)(?<!@)@<<(@.*?(?<!@))>>(.*)/) {
	        $newv .= $1 . $parms{$2};
		$vrest = $3;
		# once again, can't print error because we're not notangle
		#print STDERR "$textfile[$i]:$textline[$i]: " .
		#             "undefined parameter @<<$2@>>\n" if not $parms{$2};
	      }
	      $newv .= $vrest;
	      if($newv ne $v) {
	        # since value changed, maybe best fit changed as well
	        $new_chunk =~ s/(?<!@)\[\[$v\]\]/[[$newv]]/;
		$parms_changed = 1;
	      }
	    }
	    ($best, %best_parms) = find_best_def(idx_to_loc($i), $new_chunk) if $parms_changed;
	    foreach my $p (keys %best_parms) {
	      $new_parms{$p} = $best_parms{$p};
	    }
	    # add w/ uniquifier prefix if implicit parms referenced
	    my $has_ref = has_parmref($best, \%new_parms, \%best_parms);
	    my $sub_name = $has_ref ? $prefix . $new_chunk : $new_chunk;
            $ret .= "\@use $sub_name\n";
	    # skip if already there
	    next if $chunks{$sub_name};
	    $subdefs .= dump_def($sub_name, $sub_name, $best, %new_parms);
	  } else {
	    my $has_ref = has_parmref($best, \%parms, undef);
	    my $sub_name = $has_ref ? $prefix . $new_chunk : $new_chunk;
            $ret .= "\@use $sub_name\n";
	    # skip if already there
	    next if $chunks{$sub_name};
	    $subdefs .= dump_def($sub_name, $prefix, $new_chunk, %parms);
	  }
	}
      }
    }
  }
  return $ret . $subdefs;
}
@

<<[[nt-parm]] local definitions>>=
sub has_parmref($$$) {
  my ($chunkparts, $impl_parms, $direct_parms) = @_;
  $chunkparts = $chunks{$chunkparts};
  for(my $i = 0; $i <= $#$chunkparts; $i += 2) {
    my $low = $$chunkparts[$i];
    my $high = $$chunkparts[$i + 1];
    for(;$low <= $high; $low++) {
      if($notext[$low] =~ /^\@use (.*)$/) {
        return 1 if $$impl_parms{$1} and (not $direct_parms or not $$direct_parms{$1});
	foreach my $pass ($1 =~ /(?<!@)@<<(.*?)@>>/) {
	  return 1 if $$impl_parms{$1} and (not $direct_parms or not $$direct_parms{$1});
	}
	if($chunks{$1}) {
	  return 1 if has_parmref($1, $impl_parms, undef);
	} else {
	  my ($best, %best_parms) = find_best_def(idx_to_loc($low), $1);
	  next if not $best;
	  my %new_impl = (%$impl_parms);
	  foreach my $p (keys %best_parms) {
	    $new_impl{$p} = $best_parms{$p};
	  }
	  return 1 if has_parmref($best, \%new_impl, \%best_parms);
	}
      }
    }
  }
  return 0;
}
@

\section{Weaving}

These enhancements also require changes to the weavers.  No particular
effort will be made to typeset things nicely.  Parameter references
should be typeset as chunk references, but without the links (where
would they link to?).  To do that, they need to be hidden from the
indexer, and unhidden after the indexer.  Likewise, to get cross
references right, references to parameterized chunks with actual
parameters need to be converted to the definition format for the
indexer, and then changed back afterwards.  These tasks can be
accomplished by two filters:  one for before indexing, and one for
after.

%\lstset{language=perl}
<<nw-parm-preidx>>=
<<Common Perl Prefix>>

@

<<nw-parm-postidx>>=
<<Common Perl Prefix>>

@

The pre-index filter needs to know how to interpret chunk references
with in-line code:  either as in-line code, or as macro parameters.
The only way to do this is to know the names of all chunks.  Since
weaving does not require all tangling inputs, but this particular task
does require them, any inputs not included in the weaving process must
be specified on the filter's command line.  While it might be nice to
use the standard markup tool for this, it is not present in noweb 3.
Instead, the file is parsed directly, with a very limited view of what
constitutes the start of a chunk.  This probably needs improvement.

Much like the tangler's reader, each definition is placed into two
arrays:  one indexed by its real name, and one indexed by its name
with all parameter definitions and references stripped out.  That way,
finding the actual chunk to use for a macro reference is easier.

<<nw-parm-preidx>>=
<<[[strip_chunkname]]>>

my (%chunks, %parmchunks);

foreach my $a (@ARGV) {
#  open(my $fh, "markup $a|");
  open(my $fh, "$a");
  while(<$fh>) {
#    next unless(m/^\@defn (.*)$/);
    next unless(m/^@<<(.*)@>>=$/);
    $chunks{$1} = 1;
    my $s = strip_chunkname($1);
    $parmchunks{$s} = {} if not $parmchunks{$s};
    ${$parmchunks{$s}}{$1} = 1;
  }
  close $fh;
}
@

The filter process needs to scan the file more than once.  The first
time, it gathers definitions just like it did for the command-line
arguments.  For convenience, the file is just read into an array of
plain text lines for processing.

<<nw-parm-preidx>>=
my @file = <STDIN>;
foreach my $l (@file) {
  next unless($l =~ m/^\@defn (.*)$/);
  $chunks{$1} = 1;
  my $s = strip_chunkname($1);
  $parmchunks{$s} = {} if not $parmchunks{$s};
  ${$parmchunks{$s}}{$1} = 1;
}
@

For the second pass, an attempt is made to match a chunk reference
with a definition.  If it matches, and requires parameter expansion to
do so, it will replace its name with the parameterized definition, and
save its old name using [[@nwparmcall]].  The matching method is
copied from the tangler.  The main difference is that file and line
information are not tracked by this filter, so the location string is
always blank.

Parameter references are simply hidden away by renaming them to
[[@nwparmuse]].

As a special hack, in order to highlight parameters better, an extra
set of square brackets is placed around them.  This repeasts some of
the work that [[find_best_def]] does, but that's not too terrible.

<<nw-parm-preidx>>=
<<[[find_best_def]]>>

foreach my $l (@file) {
  if($l =~ /^\@use ([^@].*\[\[.*\]\].*)$/) {
    my ($best,) = find_best_def('', $1);
    if($best and $best ne $1) {
      my $cur = '';
      my ($b, $c) = ($best, $1);
      while($b =~  /$parm_re(.*)/) {
        my $bp = $1;
        $b = $3;
        $c =~ /^(.*?)$parm_re(.*)/;
        my $cp = $2;
        $c = $4;
	$cur .= $1;
	if($cp eq $bp) {
	  $cur .= $cp;
	} else {
	  $cur .= '[[' . $cp . ']]';
	}
      }
      $cur .= $c;
      print "\@nwparmcall $cur\n\@use $best\n";
    } else {
      print $l;
    }
  } else {
    $l =~ s/^\@use \@/\@nwparmuse \@/;
    print $l;
  }
}
@

For the third pass, done after the index, the above-added tags are
reverted.

<<nw-parm-postidx>>=
my $nwparmcall;

while(<STDIN>) {
  if(/^\@nwparmcall (.*)$/) {
    $nwparmcall = $1;
  } elsif(/^\@use / and $nwparmcall) {
    print "\@use $nwparmcall\n";
    $nwparmcall = undef;
  } else {
    s/^\@nwparmuse /\@use /;
    print "$_";
  }
}
@

\section{Other}

This is not the end of it:  [[noroots]] (noweb-2 only) needs changes
as well.  There is no reason to retain the pipeline for this tool, so
it could be standalone and not depend on the markup parser.  However,
keeping the parsing to the one official parser might be a good idea.
The [[noroots]] script is not a simple pipeline, though:  it includes
the entire ``backend'' in-line.  To make a replacement script, the
library location is lifted using a trick filter in notangle.

The procedure is the same as standard [[noroots]], except that after
collecting all usages, a pass is made over the usage array to add
parameterized usages.

<<noroots-parm>>=
<<Common Perl Prefix>>

my $LIB = `notangle -filter 'echo \$LIB >&3' /dev/null 2>/dev/null 3>&1`;
chomp $LIB;

my (%chunks, %parmchunks, %use);

open(my $f, "-|", "$LIB/markup", @ARGV);

<<[[strip_chunkname]]>>

while(<$f>) {
  if(/^\@quote$/) {
    while(<$f>) {
      last if(/\@endquote$/);
    }
  } elsif(/^\@defn (.*)$/) {
    $chunks{$1} = 1;
    my $s = strip_chunkname($1);
    $parmchunks{$s} = {} if not $parmchunks{$s};
    ${$parmchunks{$s}}{$1} = 1;
  } elsif(/^\@use (.*)$/) {
    $use{$1} = 1;
  }
}

<<[[find_best_def]]>>

for my $u (keys %use) {
  if(substr($b, 0, 1) ne '@') {
    my ($b,) = find_best_def('', $u);
    $use{$b} = 1 if $b;
  }
}
for my $d (keys %chunks) {
  print "@<<$d@>>\n" unless $use{$d};
}
@

\section{Usage}

In summary, to use this, see the introduction for syntax.  Extract the
following four chunks from this document, make them executable (or
always prefix them with the path to perl in filter command lines), and
use as described:

\begin{itemize}
\item [[nt-parm]] --- always use this as a filter when tangling.
\item [[nw-parm-preidx]] --- always use this as a filter when weaving,
before indexing (or implicit indexing, such [[-x]], [[-index]], etc.).
\item [[nw-parm-postidx]] --- always use this as a filter when
weaving, after indexing (or implicit indexing).  If no indexing is
done at all, use both of these filters with nothing in between.
\item [[noroots-parm]] --- use this in place of [[noroots]].
\end{itemize}

For example:

\begin{verbatim}
notangle -filter ./nt-parm -R'myroot' mystuff.nw > out
noweave -filter ./nw-parm-preidx -index \
        -filter ./nw-parm-postidx mystuff.nw > mystuff.tex
\end{verbatim}

If you are weaving a file that is meant to be tangled together with
other files, those other files need to be given on the
[[nw-parm-preidx]] command line.  Repeating the main input file is
harmless.  For example, if [[x.nw]] and [[y.nw]] are normally tangled
together, but weaved separately:

\begin{verbatim}
notangle -filter ./nt-parm -R'myroot' x.nw y.nw > out
noweave -filter ./nw-parm-preidx y.nw -index \
        -filter "./nw-parm-postidx y.nw" x.nw > x.tex
\end{verbatim}

\end{document}
